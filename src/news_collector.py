"""
TrendSignal MVP â€“ News Collector v3.0
Tier-vezÃ©relt, valÃ³s idejÅ±, kvÃ³ta-tudatos hÃ­rgyÅ±jtÃ©s.

StratÃ©gia (v2.0 â€“ TrendSignal_Hir_Strategia.docx):
  TIER 1 â€“ KorlÃ¡tlan, mindig fut:
    - SEC EDGAR RSS  (US 8-K, credibility 0.95)
    - Nasdaq News RSS (US ticker-specifikus, credibility 0.90)
    - BÃ‰T RSS        (HU tÅ‘zsdei kÃ¶zlemÃ©nyek, credibility 0.95)
    - Seeking Alpha  (US elemzÃ©sek, credibility 0.82)
    - Yahoo Finance  (US fallback, credibility 0.90)
    - Magyar RSS     (HU portfolio/telex/hvg, credibility 0.85)

  TIER 2 â€“ Rate-limited (60/perc), de bÅ‘sÃ©ges:
    - Finnhub

  TIER 3 â€“ Napi limit, csak ha Tier 1-2 nem elÃ©g:
    - Marketaux (batch mÃ³d: 1-2 req/ciklus, 100/nap)
    - GNews     (100/nap)

KizÃ¡rva (free tier delay):
  - NewsAPI    (akÃ¡r 1 hÃ³napos kÃ©sleltetÃ©s)
  - AlphaVantage (tÃ¶bb Ã³rÃ¡s-napos + 25 req/nap)

DeduplikÃ¡ciÃ³: URL + Jaccard title-similarity (â‰¥0.80, 30 perc ablak)

VerziÃ³: 3.0 | 2026-02-25
"""

import requests
import pandas as pd
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Optional, TYPE_CHECKING
from sqlalchemy.orm import Session
from concurrent.futures import ThreadPoolExecutor, as_completed

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

from src.config import TrendSignalConfig, get_config
from src.sentiment_analyzer import NewsItem

# Import Hungarian collector
try:
    from src.hungarian_news import HungarianNewsCollector
    HAS_HUNGARIAN = True
except ImportError:
    HAS_HUNGARIAN = False
    print("âš ï¸ Hungarian news module not available")

# Import Yahoo Finance collector
try:
    from src.yahoo_collector import YahooFinanceCollector
    HAS_YAHOO = True
except ImportError:
    HAS_YAHOO = False
    print("âš ï¸ Yahoo Finance module not available")

# Import Finnhub collector
try:
    from src.finnhub_collector import FinnhubCollector
    HAS_FINNHUB = True
except ImportError:
    HAS_FINNHUB = False
    print("âš ï¸ Finnhub module not available")

# Import Marketaux collector
try:
    from src.marketaux_collector import MarketauxCollector
    HAS_MARKETAUX = True
except ImportError:
    HAS_MARKETAUX = False
    print("âš ï¸ Marketaux module not available")

# Import GNews collector
try:
    from src.gnews_collector import GNewsCollector
    HAS_GNEWS = True
except ImportError:
    HAS_GNEWS = False
    print("âš ï¸ GNews module not available")

# Import RSS collectors (Tier 1)
try:
    from src.rss_collector import (
        SecEdgarCollector,
        NasdaqRssCollector,
        BetRssCollector,
        SeekingAlphaRssCollector,
    )
    HAS_RSS = True
except ImportError:
    HAS_RSS = False
    print("âš ï¸ RSS collector module not available")

# Import QuotaManager
try:
    from src.quota_manager import QuotaManager
    HAS_QUOTA_MANAGER = True
except ImportError:
    HAS_QUOTA_MANAGER = False
    print("âš ï¸ QuotaManager not available")

# Import BatchNewsCache
try:
    from src.batch_news_cache import BatchNewsCache
    HAS_BATCH_CACHE = True
except ImportError:
    HAS_BATCH_CACHE = False
    print("âš ï¸ BatchNewsCache not available")

if TYPE_CHECKING:
    from src.multilingual_sentiment import MultilingualSentimentAnalyzer

# Jaccard deduplikÃ¡ciÃ³ paramÃ©terei
_JACCARD_THRESHOLD = 0.80   # 80%-os hasonlÃ³sÃ¡g
_JACCARD_TIME_WINDOW = 1800  # 30 perc (mÃ¡sodperc)


class NewsCollector:
    """
    Tier-vezÃ©relt, kvÃ³ta-tudatos hÃ­rgyÅ±jtÅ‘.

    TIER 1 (korlÃ¡tlan) â†’ TIER 2 (rate-limited) â†’ TIER 3 (csak ha szÃ¼ksÃ©ges)
    Jaccard title-similarity deduplikÃ¡ciÃ³ (URL + szÃ¶veg alapjÃ¡n).
    """

    def __init__(
        self,
        config: Optional[TrendSignalConfig] = None,
        db: Optional[Session] = None,
        quota_manager: Optional['QuotaManager'] = None,
        batch_cache: Optional['BatchNewsCache'] = None,
    ):
        self.config = config or get_config()
        self.db = db

        # QuotaManager â€“ ha nincs megadva, in-memory mÃ³dban hozzuk lÃ©tre
        if quota_manager is not None:
            self.quota_manager = quota_manager
        elif HAS_QUOTA_MANAGER:
            self.quota_manager = QuotaManager(db)
        else:
            self.quota_manager = None

        # BatchNewsCache â€“ shared instance (kÃ¼lsÅ‘ scheduler injektÃ¡lhatja)
        if batch_cache is not None:
            self.batch_cache = batch_cache
        elif HAS_BATCH_CACHE:
            self.batch_cache = BatchNewsCache()
        else:
            self.batch_cache = None

        # â”€â”€ TIER 1 kollektorok â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        self.yahoo_collector = None
        if HAS_YAHOO:
            try:
                self.yahoo_collector = YahooFinanceCollector()
            except Exception as e:
                print(f"âš ï¸ Yahoo collector init failed: {e}")

        self.sec_edgar_collector = None
        self.nasdaq_rss_collector = None
        self.bet_rss_collector = None
        self.seeking_alpha_collector = None
        if HAS_RSS:
            try:
                self.sec_edgar_collector = SecEdgarCollector()
                self.nasdaq_rss_collector = NasdaqRssCollector()
                self.bet_rss_collector = BetRssCollector()
                self.seeking_alpha_collector = SeekingAlphaRssCollector()
            except Exception as e:
                print(f"âš ï¸ RSS collector init failed: {e}")

        self.hungarian_collector = None
        if HAS_HUNGARIAN:
            try:
                self.hungarian_collector = HungarianNewsCollector(config, db=self.db)
            except Exception as e:
                print(f"âš ï¸ Hungarian collector init failed: {e}")

        # â”€â”€ TIER 2 kollektorok â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        self.finnhub_collector = None
        if HAS_FINNHUB and self.config.finnhub_api_key:
            try:
                self.finnhub_collector = FinnhubCollector(self.config.finnhub_api_key)
            except Exception as e:
                print(f"âš ï¸ Finnhub collector init failed: {e}")

        # â”€â”€ TIER 3 kollektorok â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        self.marketaux_collector = None
        if HAS_MARKETAUX and self.config.marketaux_api_key:
            try:
                self.marketaux_collector = MarketauxCollector(
                    self.config.marketaux_api_key,
                    quota_manager=self.quota_manager,
                )
            except Exception as e:
                print(f"âš ï¸ Marketaux collector init failed: {e}")

        self.gnews_collector = None
        if HAS_GNEWS and self.config.gnews_api_key:
            try:
                self.gnews_collector = GNewsCollector(self.config.gnews_api_key)
            except Exception as e:
                print(f"âš ï¸ GNews collector init failed: {e}")

    # ------------------------------------------------------------------
    # PUBLIC: collect_news()
    # ------------------------------------------------------------------

    def collect_news(
        self,
        ticker_symbol: str,
        company_name: str,
        lookback_hours: int = 72,
        save_to_db: bool = True,
    ) -> List[NewsItem]:
        """
        Tier-vezÃ©relt hÃ­rgyÅ±jtÃ©s egyetlen tickerhez.

        1. TIER 1 (korlÃ¡tlan) â€“ pÃ¡rhuzamosan
        2. TIER 2 (Finnhub, rate-limited) â€“ ha van kvÃ³tÃ¡ja
        3. TIER 3 (Marketaux/GNews) â€“ csak ha Tier1+2 nem adott elÃ©g friss hÃ­rt

        Args:
            ticker_symbol: TÅ‘zsdei jelÃ¶lÅ‘ (pl. AAPL, MOL.BD)
            company_name:  CÃ©g neve (keresÃ©si kulcsszÃ³hoz)
            lookback_hours: VisszatekintÃ©si ablak
            save_to_db:    MentÃ©s DB-be (ha van session)

        Returns:
            List[NewsItem] â€“ deduplikÃ¡lt, dÃ¡tum szerint csÃ¶kkentÅ‘ sorrendben
        """
        from src.multilingual_sentiment import MultilingualSentimentAnalyzer
        sentiment_analyzer = MultilingualSentimentAnalyzer(self.config, ticker_symbol)

        is_us_ticker = not ticker_symbol.endswith('.BD')
        all_news: List[NewsItem] = []

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # TIER 1 â€“ KorlÃ¡tlan, mindig fut
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        tier1_tasks: Dict[str, callable] = {}

        if is_us_ticker:
            if self.sec_edgar_collector:
                # SEC EDGAR globÃ¡lis (1 req / Ã¶sszes ticker) â€“ egyszerÅ±sÃ­tett hÃ­vÃ¡s 1 tickerrel
                tier1_tasks['sec_edgar'] = lambda: self._collect_from_sec_edgar(
                    ticker_symbol, lookback_hours, sentiment_analyzer
                )
            if self.nasdaq_rss_collector:
                tier1_tasks['nasdaq_rss'] = lambda: self.nasdaq_rss_collector.collect_for_ticker(
                    ticker_symbol, lookback_hours, sentiment_analyzer
                )
            if self.seeking_alpha_collector:
                tier1_tasks['seeking_alpha'] = lambda: self.seeking_alpha_collector.collect_for_ticker(
                    ticker_symbol, lookback_hours, sentiment_analyzer
                )
            if self.yahoo_collector:
                tier1_tasks['yahoo'] = lambda: self._collect_from_yahoo(
                    ticker_symbol, lookback_hours, sentiment_analyzer
                )
        else:
            # BÃ‰T
            if self.bet_rss_collector:
                tier1_tasks['bet_rss'] = lambda: self._collect_from_bet(
                    ticker_symbol, lookback_hours, sentiment_analyzer
                )
            if self.hungarian_collector:
                tier1_tasks['hungarian'] = lambda: self.hungarian_collector.collect_news(
                    ticker_symbol=ticker_symbol,
                    company_name=company_name,
                    lookback_hours=lookback_hours,
                )

        if tier1_tasks:
            # Tier 1 timeout: 15 mp/forrÃ¡s â€“ RSS lassulÃ¡s vagy bot-block esetÃ©n sem akad el
            _TIER1_TASK_TIMEOUT = 15
            with ThreadPoolExecutor(max_workers=len(tier1_tasks)) as executor:
                futures = {executor.submit(fn): name for name, fn in tier1_tasks.items()}
                for future in as_completed(futures, timeout=_TIER1_TASK_TIMEOUT * 2):
                    source_name = futures[future]
                    try:
                        items = future.result(timeout=_TIER1_TASK_TIMEOUT)
                        all_news.extend(items)
                        print(f"  ğŸ“° {source_name}: {len(items)} cikk")
                    except TimeoutError:
                        print(f"  â±ï¸ {source_name} timeout ({_TIER1_TASK_TIMEOUT}s), skip")
                    except Exception as e:
                        print(f"  âš ï¸ {source_name} hiba: {e}")

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # TIER 2 â€“ Finnhub (rate-limited, 60/perc)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if is_us_ticker and self.finnhub_collector:
            can_use_finnhub = True
            if self.quota_manager:
                can_use_finnhub = self.quota_manager.can_use("finnhub")
                if can_use_finnhub:
                    self.quota_manager.record_use("finnhub")
            if can_use_finnhub:
                try:
                    finnhub_items = self._collect_from_finnhub(
                        ticker_symbol, lookback_hours, sentiment_analyzer
                    )
                    all_news.extend(finnhub_items)
                    print(f"  ğŸ“° finnhub: {len(finnhub_items)} cikk")
                except Exception as e:
                    print(f"  âš ï¸ finnhub hiba: {e}")

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # TIER 3 â€“ Marketaux / GNews (csak ha kevÃ©s friss hÃ­r van)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if is_us_ticker:
            min_fresh = getattr(self.config, 'min_fresh_news_count', 3)
            fresh_count = self._count_fresh_news(all_news, hours=2)

            if fresh_count < min_fresh:
                print(f"  â„¹ï¸ Tier 3 aktivÃ¡lÃ¡s: {fresh_count} friss hÃ­r < {min_fresh} kÃ¼szÃ¶b")

                # Marketaux batch cache ellenÅ‘rzÃ©s
                if self.marketaux_collector and self.batch_cache:
                    cached = self.batch_cache.get_for_ticker(ticker_symbol)
                    if cached:
                        all_news.extend(cached)
                        print(f"  ğŸ“° marketaux_cache: {len(cached)} cikk")
                    elif self.quota_manager is None or self.quota_manager.can_use("marketaux"):
                        marketaux_items = self._collect_from_marketaux(ticker_symbol, lookback_hours)
                        all_news.extend(marketaux_items)
                        print(f"  ğŸ“° marketaux: {len(marketaux_items)} cikk")
                elif self.marketaux_collector:
                    marketaux_items = self._collect_from_marketaux(ticker_symbol, lookback_hours)
                    all_news.extend(marketaux_items)
                    print(f"  ğŸ“° marketaux: {len(marketaux_items)} cikk")

                # GNews fallback
                elif self.gnews_collector:
                    can_use_gnews = True
                    if self.quota_manager:
                        can_use_gnews = self.quota_manager.can_use("gnews")
                        if can_use_gnews:
                            self.quota_manager.record_use("gnews")
                    if can_use_gnews:
                        try:
                            gnews_items = self._collect_from_gnews(
                                ticker_symbol, sentiment_analyzer
                            )
                            all_news.extend(gnews_items)
                            print(f"  ğŸ“° gnews: {len(gnews_items)} cikk")
                        except Exception as e:
                            print(f"  âš ï¸ gnews hiba: {e}")

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # POST-PROCESS
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        all_news = self._deduplicate_news(all_news)
        all_news.sort(key=lambda x: x.published_at, reverse=True)

        if save_to_db and self.db and all_news:
            self._save_news_to_db(all_news, ticker_symbol)

        return all_news

    # ------------------------------------------------------------------
    # TIER 1 helpers
    # ------------------------------------------------------------------

    def _collect_from_sec_edgar(
        self,
        ticker_symbol: str,
        lookback_hours: int,
        sentiment_analyzer: 'MultilingualSentimentAnalyzer',
    ) -> List[NewsItem]:
        """SEC EDGAR â€“ egyetlen ticker wrapper (globÃ¡lis feed 1 kÃ©rÃ©sbÅ‘l)."""
        try:
            result = self.sec_edgar_collector.collect(
                tickers=[ticker_symbol],
                lookback_hours=lookback_hours,
                sentiment_analyzer=sentiment_analyzer,
            )
            return result.get(ticker_symbol, [])
        except Exception as e:
            print(f"  âŒ SEC EDGAR hiba ({ticker_symbol}): {e}")
            return []

    def _collect_from_bet(
        self,
        ticker_symbol: str,
        lookback_hours: int,
        sentiment_analyzer: 'MultilingualSentimentAnalyzer',
    ) -> List[NewsItem]:
        """BÃ‰T RSS â€“ egyetlen ticker wrapper."""
        try:
            result = self.bet_rss_collector.collect(
                tickers=[ticker_symbol],
                lookback_hours=lookback_hours,
                sentiment_analyzer=sentiment_analyzer,
            )
            return result.get(ticker_symbol, [])
        except Exception as e:
            print(f"  âŒ BÃ‰T RSS hiba ({ticker_symbol}): {e}")
            return []

    def _collect_from_yahoo(
        self,
        ticker_symbol: str,
        lookback_hours: int,
        sentiment_analyzer: 'MultilingualSentimentAnalyzer',
    ) -> List[NewsItem]:
        """Yahoo Finance RSS (korlÃ¡tlan, Tier 1 fallback)."""
        try:
            news_items = self.yahoo_collector.collect_news(
                ticker_symbol=ticker_symbol,
                max_articles=20,
            )
            analyzed_items = []
            cutoff_time = datetime.now(timezone.utc) - timedelta(hours=lookback_hours)
            for item in news_items:
                if item['published_at'] < cutoff_time:
                    continue
                text = f"{item['title']}. {item.get('description', '')}"
                sentiment = sentiment_analyzer.analyze_text(text, ticker_symbol)
                analyzed_items.append(NewsItem(
                    title=item['title'],
                    description=item.get('description', ''),
                    url=item['url'],
                    published_at=item['published_at'],
                    source="Yahoo Finance",
                    sentiment_score=sentiment['score'],
                    sentiment_confidence=sentiment['confidence'],
                    sentiment_label=sentiment['label'],
                    credibility=0.90,
                ))
            return analyzed_items
        except Exception as e:
            print(f"  âŒ Yahoo Finance hiba: {e}")
            return []

    # ------------------------------------------------------------------
    # TIER 2 helpers
    # ------------------------------------------------------------------

    def _collect_from_finnhub(
        self,
        ticker_symbol: str,
        lookback_hours: int,
        sentiment_analyzer: 'MultilingualSentimentAnalyzer',
    ) -> List[NewsItem]:
        """Finnhub API (60 req/perc)."""
        try:
            lookback_days = max(1, lookback_hours // 24)
            news_items = self.finnhub_collector.collect_news(
                ticker_symbol=ticker_symbol,
                lookback_days=lookback_days,
                max_articles=20,
            )
            analyzed_items = []
            cutoff_time = datetime.now(timezone.utc) - timedelta(hours=lookback_hours)
            for item in news_items:
                if item['published_at'] < cutoff_time:
                    continue
                text = f"{item['title']}. {item.get('description', '')}"
                sentiment = sentiment_analyzer.analyze_text(text, ticker_symbol)
                analyzed_items.append(NewsItem(
                    title=item['title'],
                    description=item.get('description', ''),
                    url=item['url'],
                    published_at=item['published_at'],
                    source=f"Finnhub-{item['source']}",
                    sentiment_score=sentiment['score'],
                    sentiment_confidence=sentiment['confidence'],
                    sentiment_label=sentiment['label'],
                    credibility=0.85,
                ))
            return analyzed_items
        except Exception as e:
            print(f"  âŒ Finnhub hiba: {e}")
            return []

    # ------------------------------------------------------------------
    # TIER 3 helpers
    # ------------------------------------------------------------------

    def _collect_from_marketaux(
        self,
        ticker_symbol: str,
        lookback_hours: int,
    ) -> List[NewsItem]:
        """Marketaux API (100 req/nap, beÃ©pÃ­tett AI sentiment)."""
        try:
            lookback_days = max(1, lookback_hours // 24)
            news_items = self.marketaux_collector.collect_news(
                ticker_symbol=ticker_symbol,
                lookback_days=lookback_days,
                max_articles=20,
            )
            analyzed_items = []
            cutoff_time = datetime.now(timezone.utc) - timedelta(hours=lookback_hours)
            for item in news_items:
                if item['published_at'] < cutoff_time:
                    continue
                sentiment_score = item.get('sentiment_score', 0.0)
                if sentiment_score > 0.3:
                    label, conf = 'positive', min(abs(sentiment_score), 1.0)
                elif sentiment_score < -0.3:
                    label, conf = 'negative', min(abs(sentiment_score), 1.0)
                else:
                    label, conf = 'neutral', 0.5
                analyzed_items.append(NewsItem(
                    title=item['title'],
                    description=item.get('description', ''),
                    url=item['url'],
                    published_at=item['published_at'],
                    source=f"Marketaux-{item['source']}",
                    sentiment_score=sentiment_score,
                    sentiment_confidence=conf,
                    sentiment_label=label,
                    credibility=item.get('credibility', 0.88),
                ))
            return analyzed_items
        except Exception as e:
            print(f"  âŒ Marketaux hiba: {e}")
            return []

    def _collect_from_gnews(
        self,
        ticker_symbol: str,
        sentiment_analyzer: 'MultilingualSentimentAnalyzer',
    ) -> List[NewsItem]:
        """GNews API (100 req/nap)."""
        try:
            news_items = self.gnews_collector.collect_news(
                ticker_symbol=ticker_symbol,
                max_articles=10,
            )
            analyzed_items = []
            for item in news_items:
                text = f"{item.get('title', '')}. {item.get('description', '')}"
                sentiment = sentiment_analyzer.analyze_text(text, ticker_symbol)
                published_str = item.get('published_at', '')
                if isinstance(published_str, str):
                    try:
                        published_at = datetime.fromisoformat(
                            published_str.replace('Z', '+00:00')
                        )
                    except Exception:
                        published_at = datetime.now(timezone.utc)
                else:
                    published_at = published_str or datetime.now(timezone.utc)

                analyzed_items.append(NewsItem(
                    title=item.get('title', ''),
                    description=item.get('description', ''),
                    url=item.get('url', ''),
                    published_at=published_at,
                    source="GNews",
                    sentiment_score=sentiment['score'],
                    sentiment_confidence=sentiment['confidence'],
                    sentiment_label=sentiment['label'],
                    credibility=0.75,
                ))
            return analyzed_items
        except Exception as e:
            print(f"  âŒ GNews hiba: {e}")
            return []

    # ------------------------------------------------------------------
    # DISABLED (fizetÅ‘s upgrade esetÃ©re megÅ‘rizve)
    # ------------------------------------------------------------------

    def _collect_from_alphavantage(
        self,
        ticker_symbol: str,
        lookback_hours: int,
        sentiment_analyzer: 'MultilingualSentimentAnalyzer',
    ) -> List[NewsItem]:
        """
        DISABLED â€“ Alpha Vantage free tier: tÃ¶bb Ã³rÃ¡s-napos kÃ©sleltetÃ©s + 25 req/nap limit.
        KÃ³d megmarad fizetÅ‘s upgrade esetÃ©re (v2.0 stratÃ©gia, FÃ¡zis 1).
        NEM hÃ­vÃ³dik a collect_news() flow-bÃ³l.
        """
        url = "https://www.alphavantage.co/query"
        params = {
            'function': 'NEWS_SENTIMENT',
            'tickers': ticker_symbol,
            'apikey': self.config.alphavantage_key,
            'limit': 50,
        }
        try:
            response = requests.get(url, params=params, timeout=10)
            response.raise_for_status()
            data = response.json()
            news_items = []
            cutoff_time = datetime.now(timezone.utc) - timedelta(hours=lookback_hours)
            for item in data.get('feed', []):
                time_str = item.get('time_published', '')
                if time_str:
                    time_published = datetime.strptime(time_str, '%Y%m%dT%H%M%S')
                    time_published = time_published.replace(tzinfo=timezone.utc)
                else:
                    time_published = datetime.now(timezone.utc)
                if time_published < cutoff_time:
                    continue
                text = f"{item.get('title', '')}. {item.get('summary', '')}"
                sentiment = sentiment_analyzer.analyze_text(text, ticker_symbol)
                news_items.append(NewsItem(
                    title=item.get('title', ''),
                    description=item.get('summary', ''),
                    url=item.get('url', ''),
                    published_at=time_published,
                    source='Alpha Vantage',
                    sentiment_score=sentiment['score'],
                    sentiment_confidence=sentiment['confidence'],
                    sentiment_label=sentiment['label'],
                    credibility=0.80,
                ))
            return news_items
        except Exception as e:
            print(f"âŒ Alpha Vantage error: {e}")
            return []

    # ------------------------------------------------------------------
    # DeduplikÃ¡ciÃ³ (URL + Jaccard title-similarity)
    # ------------------------------------------------------------------

    def _deduplicate_news(self, news_items: List[NewsItem]) -> List[NewsItem]:
        """
        DuplikÃ¡ciÃ³ szÅ±rÃ©s kÃ©t szinten:
        1. Pontos URL egyezÃ©s
        2. Jaccard title-similarity â‰¥ 0.80 AND â‰¤ 30 perces idÅ‘ablak
           â†’ magasabb credibility-jÅ± marad meg
        """
        # 1. URL-alapÃº szÅ±rÃ©s
        seen_urls: set = set()
        url_unique: List[NewsItem] = []
        for item in news_items:
            if item.url and item.url not in seen_urls:
                seen_urls.add(item.url)
                url_unique.append(item)

        # 2. Jaccard title-similarity szÅ±rÃ©s
        final: List[NewsItem] = []
        for candidate in url_unique:
            is_dup = False
            for existing in final:
                if self._is_jaccard_duplicate(candidate, existing):
                    # A magasabb credibility-jÅ±t tartjuk meg
                    if candidate.credibility > existing.credibility:
                        final.remove(existing)
                        # nem breakelÃ¼nk, hÃ¡tha tÃ¶bb duplikÃ¡tumal is egyezik
                    else:
                        is_dup = True
                    break
            if not is_dup:
                final.append(candidate)

        removed = len(news_items) - len(final)
        if removed > 0:
            print(f"ğŸ”„ DeduplikÃ¡ciÃ³: {removed} duplikÃ¡tum eltÃ¡volÃ­tva (URL+Jaccard)")
        return final

    @staticmethod
    def _is_jaccard_duplicate(
        item1: NewsItem,
        item2: NewsItem,
        threshold: float = _JACCARD_THRESHOLD,
        time_window_sec: int = _JACCARD_TIME_WINDOW,
    ) -> bool:
        """
        True ha a kÃ©t cikk valÃ³szÃ­nÅ±leg ugyanaz a tartalom.

        FeltÃ©telek (mindkettÅ‘ teljesÃ¼ljÃ¶n):
        - Jaccard(title1, title2) â‰¥ threshold
        - |published_at1 - published_at2| â‰¤ time_window_sec
        """
        words1 = set(item1.title.lower().split())
        words2 = set(item2.title.lower().split())
        union = words1 | words2
        if not union:
            return False
        jaccard = len(words1 & words2) / len(union)
        if jaccard < threshold:
            return False
        # IdÅ‘ablak ellenÅ‘rzÃ©s
        try:
            time_diff = abs((item1.published_at - item2.published_at).total_seconds())
            return time_diff <= time_window_sec
        except Exception:
            return False

    # ------------------------------------------------------------------
    # SegÃ©dfÃ¼ggvÃ©nyek
    # ------------------------------------------------------------------

    def _count_fresh_news(self, news_items: List[NewsItem], hours: int = 2) -> int:
        """Az elmÃºlt `hours` Ã³rÃ¡ban megjelent cikkek szÃ¡ma."""
        cutoff = datetime.now(timezone.utc) - timedelta(hours=hours)
        return sum(1 for item in news_items if item.published_at >= cutoff)

    def _save_news_to_db(self, news_items: List[NewsItem], ticker_symbol: str):
        """HÃ­rek mentÃ©se az adatbÃ¡zisba."""
        try:
            from src.db_helpers import save_news_item_to_db
            saved_count = sum(
                1 for item in news_items
                if save_news_item_to_db(item, ticker_symbol, self.db)
            )
            if saved_count > 0:
                print(f"ğŸ’¾ Mentve: {saved_count} hÃ­r az adatbÃ¡zisba")
        except Exception as e:
            print(f"âš ï¸ DB mentÃ©s sikertelen: {e}")


if __name__ == "__main__":
    print("âœ… TrendSignal News Collector v3.0 â€“ Tier-vezÃ©relt stratÃ©gia")
    print("ğŸ“Š TIER 1: SEC EDGAR + Nasdaq RSS + BÃ‰T RSS + Seeking Alpha + Yahoo (korlÃ¡tlan)")
    print("ğŸ“Š TIER 2: Finnhub (60/perc)")
    print("ğŸ“Š TIER 3: Marketaux batch (100/nap) + GNews (100/nap) â€“ csak ha szÃ¼ksÃ©ges")
    print("ğŸš« KIZÃRVA: NewsAPI (1 hÃ³napos delay) + AlphaVantage (napos delay + 25 req/nap)")
    print("ğŸ”„ DeduplikÃ¡ciÃ³: URL + Jaccard â‰¥0.80, 30 perces ablak")
